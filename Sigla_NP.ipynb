{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMPa8PhtWmp6AdgTW5TdsMl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MASSIMOQSELLA/MyAKarpathyMicrogradTest/blob/main/Sigla_NP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WViDi5q0nTqp",
        "outputId": "971c34fe-c356-4615-a427-484293b27cde"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True vectors shape: (438, 3136)\n",
            "False vectors shape: (528, 3136)\n",
            "Dataset pronto: (966, 3136), Labels: (966, 1)\n",
            "Train set> data: (772, 3136), labels: (772, 1), True: 363, False: 409 \n",
            "Test set> data: (194, 3136), labels: (194, 1), True: 75, False: 119 \n",
            "Epoch 0/1000, Loss: 1.5250749910461123\n",
            "Epoch 100/1000, Loss: 0.6020341473806371\n",
            "Epoch 200/1000, Loss: 0.5589006290474643\n",
            "Reshuffling data at epoch 200\n",
            "Epoch 300/1000, Loss: 0.5282464194969146\n",
            "Epoch 400/1000, Loss: 0.5030221804033114\n",
            "Reshuffling data at epoch 400\n",
            "Epoch 500/1000, Loss: 0.48189907422823103\n",
            "Epoch 600/1000, Loss: 0.46344377563598965\n",
            "Reshuffling data at epoch 600\n",
            "Epoch 700/1000, Loss: 0.44698780251628245\n",
            "Epoch 800/1000, Loss: 0.4321171487346976\n",
            "Reshuffling data at epoch 800\n",
            "Epoch 900/1000, Loss: 0.41853663332536656\n",
            "Training effettuato in 390.4928 sec con 772 records\n",
            "Elapsed Time in sec: 0.0487 for 194 test records, equal to 0.0003 secs per record\n",
            "Predictions shape: (194, 1)\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import sys\n",
        "import matplotlib.pyplot as plt\n",
        "import sys\n",
        "from sklearn.model_selection import train_test_split\n",
        "import time\n",
        "from datetime import datetime\n",
        "\n",
        "#!git clone https://github.com/MASSIMOQSELLA/MyAKarpathyMicrogradTest.git\n",
        "\n",
        "def input_set(true_vectors_path, false_vectors_path): # carica i dati di input dal repository e li restituisce come immagini\n",
        "\n",
        "  # Caricamento dei dati\n",
        "  real_vectors = np.load(true_vectors_path)  # Firme vere\n",
        "  false_vectors = np.load(false_vectors_path)  # Firme false\n",
        "  print(f\"True vectors shape: {real_vectors.shape}\")  # Dovrebbe essere (438, 3136)\n",
        "  print(f\"False vectors shape: {false_vectors.shape}\")  # Dovrebbe essere (528, 3136)\n",
        "\n",
        "  # Creazione delle etichette\n",
        "  real_labels = np.ones((real_vectors.shape[0],1))  # Etichette per le firme vere (1)\n",
        "  false_labels = np.zeros((false_vectors.shape[0],1))  # Etichette per le firme false (0)\n",
        "  # Concatenazione dei dati\n",
        "  data = np.vstack((real_vectors, false_vectors))  # Combina i vettori\n",
        "  labels = np.vstack((real_labels, false_labels))  # Combina le etichette\n",
        "  # Shuffle dei dati\n",
        "  indices = np.arange(data.shape[0])\n",
        "  np.random.shuffle(indices)\n",
        "  data = data[indices]\n",
        "  labels = labels[indices]\n",
        "  print(f\"Dataset pronto: {data.shape}, Labels: {labels.shape}\")\n",
        "  # Divisione train/test\n",
        "  train_data, test_data, train_labels, test_labels = train_test_split(\n",
        "      data, labels, test_size=0.2, random_state=42\n",
        "  )\n",
        "  # conteggio false vs vere all'interno del train_set e del test_set\n",
        "  train_false_num = np.count_nonzero(train_labels == 0)\n",
        "  train_real_num = np.count_nonzero(train_labels == 1)\n",
        "  test_false_num = np.count_nonzero(test_labels == 0)\n",
        "  test_real_num = np.count_nonzero(test_labels == 1)\n",
        "  print(f\"Train set> data: {train_data.shape}, labels: {train_labels.shape}, True: {train_real_num}, False: {train_false_num} \")\n",
        "  print(f\"Test set> data: {test_data.shape}, labels: {test_labels.shape}, True: {test_real_num}, False: {test_false_num} \")\n",
        "\n",
        "  return train_data, test_data, train_labels, test_labels\n",
        "\n",
        "class SimpleNN:\n",
        "\n",
        "    def __init__(self, input_size, hidden_size, output_size, learning_rate):\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.learning_rate = learning_rate\n",
        "        # Inizializza i pesi e i bias con He per ReLU\n",
        "        #self.W1 = np.random.randn(input_size, hidden_size) * 0.01\n",
        "        self.W1 = np.random.randn(input_size, hidden_size) * np.sqrt(2. / input_size)\n",
        "        #self.b1 = np.zeros((1, hidden_size))\n",
        "        self.b1 = np.ones((1, hidden_size)) * 0.1\n",
        "        self.W2 = np.random.randn(hidden_size, output_size) * np.sqrt(2. / hidden_size)\n",
        "        self.b2 = np.zeros((1, output_size))\n",
        "        #self.b2 = np.ones((1, output_size)) * 0.001\n",
        "\n",
        "    def params(self):\n",
        "        #Restituisce i parametri (pesi e bias) del modello come un dizionario\n",
        "        return {\n",
        "            'W1': self.W1,\n",
        "            'b1': self.b1,\n",
        "            'W2': self.W2,\n",
        "            'b2': self.b2\n",
        "        }\n",
        "\n",
        "    def relu(self, Z):\n",
        "        return np.maximum(0, Z)\n",
        "\n",
        "    def relu_derivative(self, Z):\n",
        "      return Z > 0  # Restituisce un array booleano (True/False) che diventa 1/0\n",
        "\n",
        "    def sigmoid(self, Z):\n",
        "        return 1 / (1 + np.exp(-Z))\n",
        "\n",
        "    def sigmoid_derivative(self, Z):\n",
        "        return self.sigmoid(Z) * (1 - self.sigmoid(Z))\n",
        "\n",
        "    def forward(self, X):\n",
        "        # Passo forward\n",
        "        self.Z1 = np.dot(X, self.W1) + self.b1\n",
        "        self.A1 = self.relu(self.Z1)\n",
        "        self.Z2 = np.dot(self.A1, self.W2) + self.b2\n",
        "        self.A2 = self.sigmoid(self.Z2)\n",
        "        return self.A2\n",
        "\n",
        "    def compute_loss(self, X, y):\n",
        "        #Calcola la funzione di loss utilizzando la Binary Cross-Entropy (BCE)\n",
        "        predictions = self.forward(X)\n",
        "        # Aggiungi un epsilon per evitare log(0) e garantire stabilit√† numerica\n",
        "        epsilon = 1e-10\n",
        "        predictions = np.clip(predictions, epsilon, 1 - epsilon)  # Restringi i valori tra [epsilon, 1 - epsilon]\n",
        "        # Calcola la BCE\n",
        "        loss = -np.mean(y * np.log(predictions) + (1 - y) * np.log(1 - predictions))\n",
        "        return loss\n",
        "\n",
        "    def backward(self, X, y):\n",
        "\n",
        "        # Calcolare l'errore (output - vero valore)\n",
        "        m = X.shape[0]  # Numero di esempi nel batch\n",
        "\n",
        "        # Derivata della loss (BCE) rispetto all'output\n",
        "        dA2 = self.A2 - y\n",
        "\n",
        "        # Derivata della perdita rispetto ai pesi e bias dell'output\n",
        "        dZ2 = dA2 * self.sigmoid_derivative(self.Z2)\n",
        "        dW2 = np.dot(self.A1.T, dZ2) / m\n",
        "        db2 = np.sum(dZ2, axis=0, keepdims=True) / m\n",
        "\n",
        "        # Derivata della perdita rispetto ai pesi e bias del layer nascosto\n",
        "        dA1 = np.dot(dZ2, self.W2.T)\n",
        "        dZ1 = dA1 * self.relu_derivative(self.Z1)  # Derivata della ReLU\n",
        "        dW1 = np.dot(X.T, dZ1) / m\n",
        "        db1 = np.sum(dZ1, axis=0, keepdims=True) / m\n",
        "\n",
        "        # Restituisce i gradienti come dizionario\n",
        "        gradients = {\n",
        "            'W1': dW1,\n",
        "            'b1': db1,\n",
        "            'W2': dW2,\n",
        "            'b2': db2\n",
        "        }\n",
        "\n",
        "        return gradients\n",
        "\n",
        "    def train(self, X_train, y_train, epochs=1000, reshuffle_int = 200):\n",
        "        for epoch in range(epochs):\n",
        "            # Passo in avanti (forward pass)\n",
        "            #predictions = self.forward(X_train)\n",
        "\n",
        "            # Calcolare e stampare la perdita (opzionale)\n",
        "            #loss = np.mean((predictions - y_train) ** 2)\n",
        "            loss = self.compute_loss(X_train, y_train)\n",
        "\n",
        "            # Passo indietro (backward pass)\n",
        "            gradients = self.backward(X_train, y_train)\n",
        "\n",
        "            # Aggiorna i pesi e i bias con la discesa del gradiente\n",
        "            self.W1 -= self.learning_rate * gradients['W1']\n",
        "            self.b1 -= self.learning_rate * gradients['b1']\n",
        "            self.W2 -= self.learning_rate * gradients['W2']\n",
        "            self.b2 -= self.learning_rate * gradients['b2']\n",
        "\n",
        "            if epoch % 100 == 0:\n",
        "                print(f\"Epoch {epoch}/{epochs}, Loss: {loss}\")\n",
        "                # Controllo dei gradienti numerici\n",
        "                #check_gradients(self, X_train, y_train)\n",
        "\n",
        "            if (epoch % reshuffle_int == 0) and (epoch != 0):\n",
        "                print(f\"Reshuffling data at epoch {epoch}\")\n",
        "                indices = np.arange(X_train.shape[0])\n",
        "                np.random.shuffle(indices)\n",
        "                X_train = X_train[indices]\n",
        "                y_train = y_train[indices]\n",
        "\n",
        "\n",
        "def numerical_gradient(X, y, model, epsilon=1e-5):\n",
        "    \"\"\"\n",
        "    Calcola il gradiente numerico della loss rispetto ai pesi del modello\n",
        "    usando il metodo delle differenze finite.\n",
        "\n",
        "    X: dati di input\n",
        "    y: etichette vere\n",
        "    model: il modello di rete neurale\n",
        "    epsilon: piccolo valore per le differenze finite\n",
        "    \"\"\"\n",
        "    gradients = {}\n",
        "\n",
        "    # Calcola la loss originale\n",
        "    #loss_original = np.mean((model.forward(X) - y) ** 2)\n",
        "    loss_original = model.compute_loss(X, y)\n",
        "\n",
        "    # Itera sui pesi e calcola il gradiente numerico\n",
        "    for param_name, param_value in model.params().items():\n",
        "        # Salva il valore originale del parametro\n",
        "        original_value = param_value.copy()\n",
        "\n",
        "        # Perturba il parametro per calcolare la derivata\n",
        "        param_value += epsilon\n",
        "        loss_plus = model.compute_loss(X, y)\n",
        "\n",
        "        param_value -= 2 * epsilon\n",
        "        loss_minus = model.compute_loss(X, y)\n",
        "\n",
        "        # Calcola il gradiente numerico\n",
        "        gradients[param_name] = (loss_plus - loss_minus) / (2 * epsilon)\n",
        "\n",
        "        # Ripristina il parametro\n",
        "        param_value = original_value\n",
        "\n",
        "    return gradients\n",
        "\n",
        "\n",
        "def check_gradients(model, X, y, epsilon=1e-3):\n",
        "    \"\"\"\n",
        "    Confronta i gradienti calcolati dalla backpropagation con quelli\n",
        "    ottenuti tramite differenze finite.\n",
        "\n",
        "    model: il modello della rete neurale\n",
        "    X: dati di input\n",
        "    y: etichette vere\n",
        "    epsilon: piccolo valore per le differenze finite\n",
        "    \"\"\"\n",
        "    # Calcola i gradienti tramite backpropagation\n",
        "    backprop_gradients = model.backward(X, y)\n",
        "\n",
        "    # Calcola i gradienti numerici\n",
        "    numerical_gradients = numerical_gradient(X, y, model, epsilon)\n",
        "\n",
        "    # Confronta i gradienti\n",
        "    for param in model.params():\n",
        "        numerical = numerical_gradients[param]\n",
        "        backprop = backprop_gradients[param]\n",
        "\n",
        "        error = np.abs(numerical - backprop) / (np.maximum(np.abs(numerical), np.abs(backprop)) + 1e-8)\n",
        "\n",
        "        # Stampa l'errore relativo\n",
        "        if (error > 1e-6).all(): # Se almeno uno degli errori √® maggiore della soglia\n",
        "            print(f\"Errore sui gradienti per {param}: {error}\")\n",
        "        else:\n",
        "            print(f\"Gradienti per {param} passano il test.\")\n",
        "\n",
        "\n",
        "\n",
        "# testiamo la rete\n",
        "\n",
        "# Parametri della rete\n",
        "input_size = 3136\n",
        "hidden_size = 1000\n",
        "output_size = 1\n",
        "learning_rate = 0.005\n",
        "\n",
        "# Crea la rete neurale\n",
        "nn = SimpleNN(input_size, hidden_size, output_size, learning_rate)\n",
        "\n",
        "# carichiamo gli input\n",
        "#true_vectors_path = \"/content/MyAKarpathyMicrogradTest/signatures_real/dataset_sigle_real_vectors.npy\"\n",
        "#false_vectors_path = \"/content/MyAKarpathyMicrogradTest/signatures_false/dataset_sigle_false_vectors.npy\"\n",
        "\n",
        "\n",
        "train_data, test_data, train_labels, test_labels = input_set(true_vectors_path, false_vectors_path)\n",
        "\n",
        "# Allenare il modello\n",
        "st = time.time()\n",
        "nn.train(train_data, train_labels, epochs=1000)\n",
        "et = time.time() -st\n",
        "print(f\"Training effettuato in {et:.4f} sec con {train_data.shape[0]} records\")\n",
        "# Testa un batch\n",
        "st = time.time()\n",
        "test_batch = test_data  # Prendi i primi i campioni di test\n",
        "predictions = nn.forward(test_batch)\n",
        "et = time.time() -st\n",
        "print(f\"Elapsed Time in sec: {et:.4f} for {test_batch.shape[0]} test records, equal to {et/test_batch.shape[0]:.4f} secs per record\")\n",
        "print(f\"Predictions shape: {predictions.shape}\")\n",
        "#print(f\"Predictions: {predictions}\")\n"
      ]
    }
  ]
}